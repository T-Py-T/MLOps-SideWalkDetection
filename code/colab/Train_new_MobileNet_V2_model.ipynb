{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Train_new_MobileNet_V2_model.ipynb","provenance":[{"file_id":"1xCMNiRq9fqMG4zQUNU6HZFqAH2GXwA_5","timestamp":1585178240143},{"file_id":"1OhgTI1t98xVDgmwUWOVD7wOwgyRYkmaT","timestamp":1571342698340},{"file_id":"https://github.com/tensorflow/examples/blob/master/community/en/flowers_tf_lite.ipynb","timestamp":1567890810927}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hRTa3Ee15WsJ"},"source":["# Train MobileNet_V2 model with scooter image data\n"]},{"cell_type":"code","metadata":{"id":"BtgaTtZ-D8Sw","colab_type":"code","colab":{}},"source":["# Set location of training data and saved models on google drive\n","\n","model_name = 'mobileV2_LA_0409'\n","data_path=\"LA_train2\"  # training and validation data sets\n","#image_dir = 'drive/My Drive/CoLab_Data/Hollywood/'\n","image_dir = 'drive/My Drive/CoLab_Data/LosAngeles/'\n","saved_model_dir = 'drive/My Drive/CoLab_Data/save/'\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4rkDV1pl0eUH","colab_type":"code","cellView":"form","outputId":"573c5b37-53fd-4988-b18b-4eb3c1368edd","executionInfo":{"status":"ok","timestamp":1586457747383,"user_tz":360,"elapsed":56971,"user":{"displayName":"Tim Everett","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPRQ1hPu0lySQO4fY2XE-8o7JUywlvSTKiMwYUtQ=s64","userId":"01211616133197425772"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["#@title Load training data and configure workspace to train top layers of model\n","\n","# define image size\n","IMAGE_SIZE = 224\n","BATCH_SIZE = 32\n","IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n","\n","# import dependencies\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","import os\n","import numpy as np\n","from numpy.random import rand\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","from sklearn.utils import class_weight\n","from google.colab import files\n","from google.colab import drive\n","\n","# connect to Google drive\n","drive.mount('/content/drive')\n","\n","# unzip training images to local colab drive\n","os.system('unzip \"%s%s.zip\"' % (image_dir, data_path))\n","\n","# Add random patch over 1/9 of image 50% of the time to training image generator\n","def addPatch(img):\n","  x = int(rand(1)[0]*3) * IMAGE_SIZE//3 \n","  y = int(rand(1)[0]*3) * IMAGE_SIZE//3 \n","  w = IMAGE_SIZE//3\n","  h = IMAGE_SIZE//3\n","  if rand(1)[0] > 0.5:\n","    for c in range(3):\n","      img[x:x+w,y:y+h,c]=255*rand(w,h)     \n","  return(img)\n","\n","# Use ImageDataGenerator to rescale and augment the images.\n","data_path_train = data_path + '/train'\n","data_path_val = data_path + '/val'\n","datagen_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n","    horizontal_flip=True,\n","    brightness_range=[0.8,1.2],\n","    width_shift_range=[-0.15,0.15],\n","    height_shift_range=[-0.15,0.15],\n","    zoom_range=[0.9,1.1],\n","    rotation_range=10,\n","    preprocessing_function=addPatch) \n","\n","datagen_val = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n","\n","train_generator = datagen_train.flow_from_directory(\n","    data_path_train,\n","    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n","    batch_size=BATCH_SIZE,\n","    shuffle=True)\n","\n","val_generator = datagen_val.flow_from_directory(\n","    data_path_val,\n","    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n","    batch_size=BATCH_SIZE)\n","\n","# Setup callback to save intermediate results to Google drive every epoch\n","checkpoint_path = saved_model_dir + '/' + model_name + '_top/' +\"cp-{epoch:04d}.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path, \n","    verbose=1, \n","    save_weights_only=True,\n","    save_freq='epoch')\n","\n","# Define class labels\n","labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n","\n","# Configure data generators\n","steps_per_epoch = train_generator.n // BATCH_SIZE\n","validation_steps = val_generator.n // BATCH_SIZE\n","class_weights = class_weight.compute_class_weight(\n","               'balanced',\n","                np.unique(train_generator.classes), \n","                train_generator.classes)\n","\n","#class_weights = [1,class_weights[0], class_weights[1], class_weights[2]] # temp till get bike_lane and crosswalk data\n","#class_weights = [1, 1, class_weights[0], class_weights[1]] # temp till get bike_lane and crosswalk data\n","class_weights = dict(enumerate(class_weights))   \n","print(class_weights)\n","\n","# Create the base model from the pre-trained model MobileNetV2\n","base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n","                                              include_top=False, layers=tf.keras.layers, \n","                                              weights='imagenet')\n","base_model.trainable = False\n","\n","# Add new top layer and compile\n","model = tf.keras.Sequential([\n","  base_model,\n","#  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n","#  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.GlobalAveragePooling2D(),\n","  tf.keras.layers.Dense(4, activation='softmax')  # first number specifies number of classes\n","])\n","model.compile(optimizer=tf.keras.optimizers.Adam(), \n","              loss='categorical_crossentropy', \n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","Found 23973 images belonging to 4 classes.\n","Found 9338 images belonging to 4 classes.\n","{0: 2.9079330422125182, 1: 3.8566602316602316, 2: 0.627959974853311, 3: 0.5542121324209358}\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","9412608/9406464 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ygYRVRtn3ZK7","colab_type":"text"},"source":["# Either train top layers of model or load previous checkpoint"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iBMcobPHdD8O","outputId":"0189d723-5f7d-472f-f0e0-4dbea3370e47","executionInfo":{"status":"ok","timestamp":1586459345454,"user_tz":360,"elapsed":1575391,"user":{"displayName":"Tim Everett","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPRQ1hPu0lySQO4fY2XE-8o7JUywlvSTKiMwYUtQ=s64","userId":"01211616133197425772"}},"cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["#Train top layers of model \n","\n","epochs = 5\n","\n","# Initial training: Train just the top few layers of the model\n","history = model.fit(train_generator,\n","                    steps_per_epoch = steps_per_epoch, \n","                    epochs=epochs,\n","                    callbacks=[cp_callback], \n","                    validation_data=val_generator,\n","                    validation_steps=validation_steps,\n","                    class_weight=class_weights)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","749/749 [==============================] - ETA: 0s - loss: 0.6271 - accuracy: 0.7389\n","Epoch 00001: saving model to drive/My Drive/CoLab_Data/save//mobileV2_LA_0409_top/cp-0001.ckpt\n","749/749 [==============================] - 318s 425ms/step - loss: 0.6271 - accuracy: 0.7389 - val_loss: 0.4239 - val_accuracy: 0.8429\n","Epoch 2/5\n","749/749 [==============================] - ETA: 0s - loss: 0.4852 - accuracy: 0.7986\n","Epoch 00002: saving model to drive/My Drive/CoLab_Data/save//mobileV2_LA_0409_top/cp-0002.ckpt\n","749/749 [==============================] - 312s 416ms/step - loss: 0.4852 - accuracy: 0.7986 - val_loss: 0.4932 - val_accuracy: 0.8210\n","Epoch 3/5\n","749/749 [==============================] - ETA: 0s - loss: 0.4536 - accuracy: 0.8130\n","Epoch 00003: saving model to drive/My Drive/CoLab_Data/save//mobileV2_LA_0409_top/cp-0003.ckpt\n","749/749 [==============================] - 312s 417ms/step - loss: 0.4536 - accuracy: 0.8130 - val_loss: 0.3897 - val_accuracy: 0.8570\n","Epoch 4/5\n","749/749 [==============================] - ETA: 0s - loss: 0.4344 - accuracy: 0.8231\n","Epoch 00004: saving model to drive/My Drive/CoLab_Data/save//mobileV2_LA_0409_top/cp-0004.ckpt\n","749/749 [==============================] - 312s 416ms/step - loss: 0.4344 - accuracy: 0.8231 - val_loss: 0.4151 - val_accuracy: 0.8499\n","Epoch 5/5\n","749/749 [==============================] - ETA: 0s - loss: 0.4242 - accuracy: 0.8256\n","Epoch 00005: saving model to drive/My Drive/CoLab_Data/save//mobileV2_LA_0409_top/cp-0005.ckpt\n","749/749 [==============================] - 312s 416ms/step - loss: 0.4242 - accuracy: 0.8256 - val_loss: 0.3839 - val_accuracy: 0.8573\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Mg3fE7R3isB","colab_type":"code","outputId":"f68be3ff-2fa5-4620-8112-e139605c4246","executionInfo":{"status":"ok","timestamp":1586313954154,"user_tz":360,"elapsed":31913,"user":{"displayName":"Tim Everett","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPRQ1hPu0lySQO4fY2XE-8o7JUywlvSTKiMwYUtQ=s64","userId":"01211616133197425772"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["checkpoint = 'cp-0020.ckpt'\n","checkpoint_path = 'drive/My Drive/CoLab_Data/save/mobileV2_LA_0402_rev1'\n","model.load_weights(checkpoint_path + '/' + checkpoint)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1c097e7f98>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"KyA4VAipTHyx","colab_type":"code","outputId":"d68b3878-7467-4489-83c0-6a78e862c0dd","executionInfo":{"status":"ok","timestamp":1586468800442,"user_tz":360,"elapsed":1146,"user":{"displayName":"Tim Everett","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPRQ1hPu0lySQO4fY2XE-8o7JUywlvSTKiMwYUtQ=s64","userId":"01211616133197425772"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["# Configure model for fine tuning\n","\n","# Unfreeze top layers of model\n","base_model.trainable = True\n","fine_tune_at = 100    \n","# Freeze all the layers before the `fine_tune_at` layer\n","for layer in base_model.layers[:fine_tune_at]:\n","  layer.trainable =  False\n","\n","BATCH_SIZE = 128\n","\n","# Compile model\n","model.compile(loss='categorical_crossentropy',\n","              optimizer = tf.keras.optimizers.Adam(lr=0.00002),\n","              metrics=['accuracy'])\n","model.summary()\n","\n","# Setup data generators \n","steps_per_epoch = train_generator.n // BATCH_SIZE\n","validation_steps = val_generator.n // BATCH_SIZE\n","\n","datagen_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n","    horizontal_flip=True,\n","    brightness_range=[0.8,1.2],\n","    width_shift_range=[-0.15,0.15],\n","    height_shift_range=[-0.15,0.15],\n","    zoom_range=[0.9,1.1],\n","    rotation_range=25, \n","    preprocessing_function=addPatch)\n","\n","train_generator = datagen_train.flow_from_directory(\n","    data_path_train,\n","    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n","    batch_size=BATCH_SIZE,\n","    shuffle=True)\n","\n","# Setup callback to save intermediate results to Google drive every epoch\n","checkpoint_path = saved_model_dir + '/' + model_name + '/' +\"cp-{epoch:04d}.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path, \n","    verbose=1, \n","    save_weights_only=True,\n","    save_freq='epoch')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","mobilenetv2_1.00_224 (Model) (None, 7, 7, 1280)        2257984   \n","_________________________________________________________________\n","global_average_pooling2d (Gl (None, 1280)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 4)                 5124      \n","=================================================================\n","Total params: 2,263,108\n","Trainable params: 1,867,716\n","Non-trainable params: 395,392\n","_________________________________________________________________\n","Found 23973 images belonging to 4 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dXBwdmeYVcKx","colab_type":"code","colab":{}},"source":["# Perform fine tuning\n","\n","from sklearn.utils import class_weight\n","\n","epochs=30\n","\n","history_fine = model.fit(train_generator,\n","                         steps_per_epoch = steps_per_epoch,           \n","                         epochs=epochs,\n","                         callbacks=[cp_callback],\n","                         validation_data=val_generator,\n","                         validation_steps=validation_steps,\n","                         class_weight=class_weights)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0d716TetEHkE","colab_type":"code","colab":{}},"source":[" # Convert to TFlite, save and download results\n","\n"," # Save model \n","model.save(model_name + '.h5')\n","model.save(saved_model_dir + model_name + '.h5')\n","tf.saved_model.save(model, model_name)\n","\n","# convert to tflite, original (pre2.0) method\n","#converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n","#tflite_model = converter.convert()\n","\n","####### Convert for 2.0 using concrete functions #####\n","# Get the concrete function from the Keras model.\n","run_model = tf.function(lambda x : model(x))\n","\n","# Save the concrete function.\n","concrete_func = run_model.get_concrete_function(\n","    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\n","\n","# Convert the model to tflite\n","converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n","tflite_model = converter.convert()\n","#open(TFLITE_MODEL, \"wb\").write(tflite_model)\n","\n","\n","\n","# save tflite model to Google drive and download folder on local drive \n","with open(saved_model_dir + model_name + '.tflite', 'wb') as f:\n","  f.write(tflite_model)\n","with open(model_name + '.tflite', 'wb') as f:\n","  f.write(tflite_model)\n","files.download(model_name + '.tflite')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8wBBv3Bh9gu","colab_type":"code","colab":{}},"source":["\n","\n","files.download(model_name + '.tflite')"],"execution_count":0,"outputs":[]}]}