{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train_sidewalk_coral.ipynb","version":"0.3.2","provenance":[{"file_id":"1lSbmTUGHKk8xoanaU-ejqwVh85xjizxN","timestamp":1561756150020}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XgMTTqSamuzB","colab_type":"code","outputId":"c6db05b4-e801-48f4-9a43-6fced0a418dc","executionInfo":{"status":"ok","timestamp":1561767922608,"user_tz":360,"elapsed":31839,"user":{"displayName":"Tim Everett","photoUrl":"https://lh3.googleusercontent.com/-7HgMHsW7KMk/AAAAAAAAAAI/AAAAAAAACSg/7HgHOhgaudw/s64/photo.jpg","userId":"01211616133197425772"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!sudo apt update  \n","!sudo apt install apt-transport-https ca-certificates curl software-properties-common\n","!curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n","!sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\"\n","!sudo apt update\n","!sudo apt install docker-ce\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\u001b[33m\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connected to deve\u001b[0m\r                                                                               \rHit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n","\r                                                                               \rHit:3 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","\u001b[33m\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Waiting for heade\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to cloud.r-proje\u001b[0m\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Connecting to cloud.r-project.org] [Waiting for h\u001b[0m\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Connecting to cloud.r-project.org] [Waiting for h\u001b[0m\r                                                                               \rHit:6 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n","\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Connecting to cloud.r-project.org] [Waiting for h\u001b[0m\u001b[33m\r                                                                               \r0% [Connected to cloud.r-project.org (52.222.157.20)] [Waiting for headers]\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 88.7 kB] [Connected to cloud.r-project.org (52.222.157.20)\u001b[0m\r                                                                               \rIgn:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n","Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ca-certificates is already the newest version (20180409).\n","curl is already the newest version (7.58.0-2ubuntu3.7).\n","software-properties-common is already the newest version (0.96.24.32.9).\n","apt-transport-https is already the newest version (1.6.11).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-410\n","Use 'sudo apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n","OK\n","Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n","Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:3 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Hit:6 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n","Hit:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n","Get:8 https://download.docker.com/linux/ubuntu bionic InRelease [64.4 kB]\n","Ign:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:14 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages [6,974 B]\n","Fetched 71.4 kB in 1s (84.3 kB/s)\n","Reading package lists... Done\n","Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n","Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:3 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Hit:5 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n","Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Hit:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n","Hit:8 https://download.docker.com/linux/ubuntu bionic InRelease\n","Ign:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-410\n","Use 'sudo apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  apparmor aufs-tools cgroupfs-mount containerd.io docker-ce-cli iptables\n","  libip6tc0 libiptc0 libmnl0 libnetfilter-conntrack3 libnfnetlink0\n","  libxtables12 pigz\n","Suggested packages:\n","  apparmor-profiles-extra apparmor-utils\n","The following NEW packages will be installed:\n","  apparmor aufs-tools cgroupfs-mount containerd.io docker-ce docker-ce-cli\n","  iptables libip6tc0 libiptc0 libmnl0 libnetfilter-conntrack3 libnfnetlink0\n","  libxtables12 pigz\n","0 upgraded, 14 newly installed, 0 to remove and 41 not upgraded.\n","Need to get 54.2 MB of archives.\n","After this operation, 257 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnfnetlink0 amd64 1.0.1-3 [13.3 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1 [57.4 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmnl0 amd64 1.0.4-2 [12.3 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxtables12 amd64 1.6.1-2ubuntu2 [27.9 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 apparmor amd64 2.12-4ubuntu5.1 [487 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libip6tc0 amd64 1.6.1-2ubuntu2 [19.9 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libiptc0 amd64 1.6.1-2ubuntu2 [9,308 B]\n","Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnetfilter-conntrack3 amd64 1.0.6-2 [37.8 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 iptables amd64 1.6.1-2ubuntu2 [269 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 aufs-tools amd64 1:4.9+20170918-1ubuntu1 [104 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cgroupfs-mount all 1.4 [6,320 B]\n","Get:12 https://download.docker.com/linux/ubuntu bionic/stable amd64 containerd.io amd64 1.2.6-3 [22.6 MB]\n","Get:13 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-cli amd64 5:18.09.7~3-0~ubuntu-bionic [13.2 MB]\n","Get:14 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce amd64 5:18.09.7~3-0~ubuntu-bionic [17.4 MB]\n","Fetched 54.2 MB in 2s (33.1 MB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 14.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libnfnetlink0:amd64.\n","(Reading database ... 130946 files and directories currently installed.)\n","Preparing to unpack .../00-libnfnetlink0_1.0.1-3_amd64.deb ...\n","Unpacking libnfnetlink0:amd64 (1.0.1-3) ...\n","Selecting previously unselected package pigz.\n","Preparing to unpack .../01-pigz_2.4-1_amd64.deb ...\n","Unpacking pigz (2.4-1) ...\n","Selecting previously unselected package libmnl0:amd64.\n","Preparing to unpack .../02-libmnl0_1.0.4-2_amd64.deb ...\n","Unpacking libmnl0:amd64 (1.0.4-2) ...\n","Selecting previously unselected package libxtables12:amd64.\n","Preparing to unpack .../03-libxtables12_1.6.1-2ubuntu2_amd64.deb ...\n","Unpacking libxtables12:amd64 (1.6.1-2ubuntu2) ...\n","Selecting previously unselected package apparmor.\n","Preparing to unpack .../04-apparmor_2.12-4ubuntu5.1_amd64.deb ...\n","Unpacking apparmor (2.12-4ubuntu5.1) ...\n","Selecting previously unselected package libip6tc0:amd64.\n","Preparing to unpack .../05-libip6tc0_1.6.1-2ubuntu2_amd64.deb ...\n","Unpacking libip6tc0:amd64 (1.6.1-2ubuntu2) ...\n","Selecting previously unselected package libiptc0:amd64.\n","Preparing to unpack .../06-libiptc0_1.6.1-2ubuntu2_amd64.deb ...\n","Unpacking libiptc0:amd64 (1.6.1-2ubuntu2) ...\n","Selecting previously unselected package libnetfilter-conntrack3:amd64.\n","Preparing to unpack .../07-libnetfilter-conntrack3_1.0.6-2_amd64.deb ...\n","Unpacking libnetfilter-conntrack3:amd64 (1.0.6-2) ...\n","Selecting previously unselected package iptables.\n","Preparing to unpack .../08-iptables_1.6.1-2ubuntu2_amd64.deb ...\n","Unpacking iptables (1.6.1-2ubuntu2) ...\n","Selecting previously unselected package aufs-tools.\n","Preparing to unpack .../09-aufs-tools_1%3a4.9+20170918-1ubuntu1_amd64.deb ...\n","Unpacking aufs-tools (1:4.9+20170918-1ubuntu1) ...\n","Selecting previously unselected package cgroupfs-mount.\n","Preparing to unpack .../10-cgroupfs-mount_1.4_all.deb ...\n","Unpacking cgroupfs-mount (1.4) ...\n","Selecting previously unselected package containerd.io.\n","Preparing to unpack .../11-containerd.io_1.2.6-3_amd64.deb ...\n","Unpacking containerd.io (1.2.6-3) ...\n","Selecting previously unselected package docker-ce-cli.\n","Preparing to unpack .../12-docker-ce-cli_5%3a18.09.7~3-0~ubuntu-bionic_amd64.deb ...\n","Unpacking docker-ce-cli (5:18.09.7~3-0~ubuntu-bionic) ...\n","Selecting previously unselected package docker-ce.\n","Preparing to unpack .../13-docker-ce_5%3a18.09.7~3-0~ubuntu-bionic_amd64.deb ...\n","Unpacking docker-ce (5:18.09.7~3-0~ubuntu-bionic) ...\n","Setting up aufs-tools (1:4.9+20170918-1ubuntu1) ...\n","Setting up containerd.io (1.2.6-3) ...\n","Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /lib/systemd/system/containerd.service.\n","Setting up libiptc0:amd64 (1.6.1-2ubuntu2) ...\n","Setting up apparmor (2.12-4ubuntu5.1) ...\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n","debconf: falling back to frontend: Readline\n","Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n","Setting up cgroupfs-mount (1.4) ...\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Setting up libxtables12:amd64 (1.6.1-2ubuntu2) ...\n","Processing triggers for systemd (237-3ubuntu10.22) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Setting up libnfnetlink0:amd64 (1.0.1-3) ...\n","Setting up docker-ce-cli (5:18.09.7~3-0~ubuntu-bionic) ...\n","Setting up libmnl0:amd64 (1.0.4-2) ...\n","Setting up pigz (2.4-1) ...\n","Setting up libip6tc0:amd64 (1.6.1-2ubuntu2) ...\n","Setting up libnetfilter-conntrack3:amd64 (1.0.6-2) ...\n","Setting up iptables (1.6.1-2ubuntu2) ...\n","Setting up docker-ce (5:18.09.7~3-0~ubuntu-bionic) ...\n","update-alternatives: using /usr/bin/dockerd-ce to provide /usr/bin/dockerd (dockerd) in auto mode\n","Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /lib/systemd/system/docker.service.\n","Created symlink /etc/systemd/system/sockets.target.wants/docker.socket → /lib/systemd/system/docker.socket.\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Processing triggers for systemd (237-3ubuntu10.22) ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6VxWZ-zQn6xx","colab_type":"code","outputId":"f4793d2d-8e8a-4266-be45-7d9c9c8214ee","executionInfo":{"status":"ok","timestamp":1561767962895,"user_tz":360,"elapsed":2013,"user":{"displayName":"Tim Everett","photoUrl":"https://lh3.googleusercontent.com/-7HgMHsW7KMk/AAAAAAAAAAI/AAAAAAAACSg/7HgHOhgaudw/s64/photo.jpg","userId":"01211616133197425772"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!sudo systemctl status docker\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["System has not been booted with systemd as init system (PID 1). Can't operate.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9fIjk3ksbLQb","colab_type":"code","colab":{}},"source":["# Load training data to colab space\n","\n","# mount local drive: works with CPU, not with GPU\n","#from google.colab import drive\n","#drive.mount('/content/drive/')\n","\n","# load file from specific link, didn't work for me\n","#download=drive.CreateFile({'id':'https://drive.google.com/open?id=1-s9pfnlG8Ln7eJRfJhr1PlX17Kzou5jW'})\n","#download.GetContentFile('training3.zip')\n","#!unzip training3.zip\n","\n","# generic link to local drive, need to specify file through GUI \n","#from google.colab import files\n","#files.upload()\n","#import os\n","#os.rename('../training5.zip', 'training5.zip')\n","!unzip training5.zip\n","\n","\n","# or just upload from Files menu to left"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRqXuUQ5qx4Q","colab_type":"code","outputId":"f311d313-41bd-4776-c958-c615bc9a9647","executionInfo":{"status":"ok","timestamp":1561756469181,"user_tz":360,"elapsed":5466,"user":{"displayName":"Tim Everett","photoUrl":"https://lh3.googleusercontent.com/-7HgMHsW7KMk/AAAAAAAAAAI/AAAAAAAACSg/7HgHOhgaudw/s64/photo.jpg","userId":"01211616133197425772"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["!pip install -q keras\n","import keras\n","print(keras.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["2.2.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WtbAFCZrrGMC","colab_type":"code","outputId":"ef33ddc2-cf07-442f-e1ec-b083e369be79","executionInfo":{"status":"error","timestamp":1561757335671,"user_tz":360,"elapsed":285531,"user":{"displayName":"Tim Everett","photoUrl":"https://lh3.googleusercontent.com/-7HgMHsW7KMk/AAAAAAAAAAI/AAAAAAAACSg/7HgHOhgaudw/s64/photo.jpg","userId":"01211616133197425772"}},"colab":{"base_uri":"https://localhost:8080/","height":924}},"source":["# train only top classification layer, base model frozen\n","\n","import keras\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.preprocessing import image\n","from keras.applications import MobileNetV2\n","from keras.applications.mobilenet import preprocess_input\n","from keras import backend as K\n","import numpy as np\n","import os\n","\n","\n","# config parameters\n","image_size = 160 # All images will be resized to 160x160\n","batch_size = 32\n","base_dir='training5'\n","\n","IMG_SHAPE = (image_size, image_size, 3)\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')\n","\n","# Rescale all images by 1./255 and apply image augmentation\n","train_datagen = keras.preprocessing.image.ImageDataGenerator(\n","                rescale=1./255,\n","                rotation_range=10,\n","                width_shift_range=.05,\n","                height_shift_range=0.05)\n","\n","validation_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n","\n","# Flow training images in batches using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","                train_dir,  # Source directory for the training images\n","                target_size=(image_size, image_size),  \n","                batch_size=batch_size,\n","                # Since we use binary_crossentropy loss, we need binary labels\n","                class_mode='binary')\n","\n","# Flow validation images in batches using test_datagen generator\n","validation_generator = validation_datagen.flow_from_directory(\n","                validation_dir, # Source directory for the validation images\n","                target_size=(image_size, image_size),\n","                batch_size=batch_size,\n","                class_mode='binary')\n","\n","\n","# get pre-trained model\n","base_model=MobileNetV2(weights='imagenet',\n","                       include_top=False,\n","                       input_shape=IMG_SHAPE)\n","base_model.trainable=False\n","\n","# add layers to model\n","model = keras.Sequential([\n","  base_model,\n","  keras.layers.GlobalAveragePooling2D(),\n","  keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","\n","# compile model\n","#model.compile(optimizer=keras.optimizers.RMSprop(lr=0.0001), \n","#model.compile(optimizer=keras.optimizers.RMSprop(lr=0.001), \n","model.compile(optimizer='Adam',               \n","              loss='binary_crossentropy', \n","              metrics=['accuracy'])\n","\n","# train model\n","epochs = 10\n","steps_per_epoch = train_generator.n // batch_size\n","validation_steps = validation_generator.n // batch_size\n","\n","history = model.fit_generator(train_generator, \n","                              steps_per_epoch = steps_per_epoch,\n","                              epochs=epochs, \n","                              workers=4,\n","                              #use_multiprocessing=True ,\n","                              validation_data=validation_generator, \n","                              validation_steps=validation_steps)\n","\n","\n","#save results\n","model.save('sw_street_v2_15epoch_p001.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 21414 images belonging to 2 classes.\n","Found 3504 images belonging to 2 classes.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0628 21:24:09.369825 140033442740096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0628 21:24:09.410164 140033442740096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0628 21:24:09.422384 140033442740096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0628 21:24:09.455877 140033442740096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","W0628 21:24:09.457960 140033442740096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","W0628 21:24:12.193477 140033442740096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5\n","9412608/9406464 [==============================] - 1s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["W0628 21:24:25.367329 140033442740096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0628 21:24:25.394745 140033442740096 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","669/669 [==============================] - 113s 168ms/step - loss: 0.3319 - acc: 0.8576 - val_loss: 0.3846 - val_acc: 0.8320\n","Epoch 2/10\n","669/669 [==============================] - 103s 154ms/step - loss: 0.2562 - acc: 0.8934 - val_loss: 0.3337 - val_acc: 0.8618\n","Epoch 3/10\n","362/669 [===============>..............] - ETA: 44s - loss: 0.2368 - acc: 0.9046"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-816135737e4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m                               \u001b[0;31m#use_multiprocessing=True ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ipNJXUyw_iZ8","colab_type":"code","colab":{}},"source":["#save results\n","model.save('coral_test.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k40qvWrwf-cQ","colab_type":"code","colab":{}},"source":["# plot training history\n","\n","import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,max(plt.ylim())])\n","plt.title('Training and Validation Loss')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLX0P2Lmifp_","colab_type":"code","colab":{}},"source":["# Modify model to make top 55 (of 155) layers trainable\n","\n","base_model.trainable = True\n","\n","# Let's take a look to see how many layers are in the base model\n","print(\"Number of layers in the base model: \", len(base_model.layers))\n","\n","# Fine tune from this layer onwards\n","fine_tune_at = 55\n","\n","# Freeze all the layers before the `fine_tune_at` layer\n","for layer in base_model.layers[:fine_tune_at]:\n","  layer.trainable =  False\n","  \n","model.compile(loss='binary_crossentropy',\n","              optimizer=keras.optimizers.Adam(lr=0.00002),\n","              metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"meqKDmwCijo0","colab_type":"code","colab":{}},"source":["# do fine tuning of top layers specified above\n","epochs=5\n","history_fine = model.fit_generator(train_generator, \n","                                   steps_per_epoch = steps_per_epoch,\n","                                   epochs=epochs, \n","                                   workers=4,\n","                                   validation_data=validation_generator, \n","                                   validation_steps=validation_steps)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lP6rIYmXwF9b","colab_type":"code","colab":{}},"source":["# look at prediction errors\n","\n","import matplotlib.pyplot as plt\n","from keras.preprocessing.image import load_img\n","\n","\n","# Get the filenames from the generator\n","fnames = validation_generator.filenames\n"," \n","# Get the ground truth from generator\n","ground_truth = validation_generator.classes\n"," \n","# Get the label to class mapping from the generator\n","label2index = validation_generator.class_indices\n"," \n","# Getting the mapping from class index to class label\n","idx2label = dict((v,k) for k,v in label2index.items())\n"," \n","# Get the predictions from the model using the generator\n","predictions = model.predict_generator(validation_generator,\n","                                      steps=validation_generator.samples/validation_generator.batch_size,\n","                                      verbose=1)\n","predictions=np.squeeze(predictions)\n","#predicted_classes = np.argmax(predictions,axis=1)\n","predicted_classes=predictions.astype(int)\n"," \n","errors = np.where(predicted_classes != ground_truth)\n","print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))\n"," \n","# Show the errors\n","for i in range(100):\n","    #pred_class = np.argmax(predictions[errors[i]])\n","    pred_class=predicted_classes[errors[i]]\n","    pred_label = idx2label[pred_class]\n","     \n","    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n","        fnames[errors[i]].split('/')[0],\n","        pred_label,\n","        predictions[errors[i]][0])\n","     \n","    original = load_img('{}/{}'.format(validation_dir,fnames[errors[i]]))\n","    plt.figure(figsize=[7,7])\n","    plt.axis('off')\n","    plt.title(title)\n","    plt.imshow(original)\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"72f_diLQLelW","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"X40egaEk8Non","colab_type":"code","colab":{}},"source":["#save results\n","model.save('mobileV2_5epochs_100layer_0516.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZqVoRguONMe","colab_type":"code","colab":{}},"source":["\n","\n","import numpy as np\n","import cv2\n","from keras.applications.mobilenet import preprocess_input\n","from keras.preprocessing import image\n","from keras.models import load_model\n","import os\n","import matplotlib.pyplot as plt\n","from keras.preprocessing.image import load_img\n","\n","dirIn = 'training5/train/street'\n","modelName='mobileV2_20epochs_0516.h5'\n","loadModel=0\n","\n","\n","if loadModel:\n","    model=load_model(modelName)\n","else:\n","    print('Model not loaded')\n","    \n","\n","\n","\n","# loop through all images in the folder\n","preds=[]\n","fnum=0\n","for file in os.listdir(dirIn):\n","    if file[-3:]!='jpg':\n","      continue\n","    frame= cv2.imread(os.path.join(dirIn,file))\n","    fnum+=1\n","    #print(fnum)\n","   \n","    # pre-process image\n","    img = cv2.resize(frame, (160,160), interpolation = cv2.INTER_AREA)\n","      \n","    # convert to RGB\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img=np.divide(img,255)\n","\n","    x = np.expand_dims(img, axis=0)\n","\n","    #x = preprocess_input(x)\n","    \n","    # classify image\n","    preds.append(model.predict(x)[0][0])\n","    if preds[-1]<0.5:\n","      original = load_img('{}/{}'.format(dirIn,file))\n","      plt.figure(figsize=[7,7])\n","      plt.axis('off')\n","      plt.title(file)\n","      plt.imshow(original)\n","      plt.show()\n","      \n","\n","  \n","\n","p=np.asarray(preds)    \n","errors=np.where(p>0.5)\n","\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1N6VUw6Jvi3","colab_type":"code","colab":{}},"source":["import os\n","import tensorflow as tf\n","# This address identifies the TPU we'll use when configuring TensorFlow.\n","TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","tf.logging.set_verbosity(tf.logging.INFO)\n","\n","tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n","    base_model,\n","    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n","        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n"],"execution_count":0,"outputs":[]}]}